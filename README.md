# Cross-Subject Imagined Speech EEG-to-Text Decoder

**Cross-subject contrastive learning system for sentence-level imagined speech decoding**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)

---

## üéØ Key Achievement

Achieved **17% top-1 accuracy** on 200-way EEG-to-text retrieval (34x better than chance) with cross-subject training on ChiSCO imagined speech dataset subjects S1 and S2, despite inter-subject correlation of only **r=0.13**.

**Why this matters:** Most EEG decoders require 900+ minutes of per-user calibration. This cross-subject approach demonstrates that subject-invariant features can be learned, potentially reducing calibration requirements from hours to minutes‚Äîmaking BCIs more practical for real-world deployment.

### Research Context

**To the best of our knowledge**, no prior work demonstrates cross-subject sentence-level imagined speech decoding using contrastive retrieval. This was developed as an internal MVP for Excelleve rather than a published benchmark, but we validated against existing literature to confirm the gap.

### Built on ChiSCO Dataset

This work builds upon the **ChiSCO (Chinese Imagined Speech Corpus)** dataset created by Zhang et al. (2024) at Harbin Institute of Technology‚Äîthe first dataset for sentence-level imagined speech with 6,681 trials per subject. The ChiSCO team's rigorous data collection (900+ minutes per subject, attention checks, quality controls) and their pioneering work on imagined speech paradigms made this cross-subject research possible.

---

## üìä Performance Metrics

### Test Set Performance (S1+S2, 200 samples)

| Metric | Value | Baseline |
|--------|-------|----------|
| **Top-1 Accuracy** | 17.0% | 0.5% (chance) |
| **Top-5 Accuracy** | 47.5% | 2.5% (chance) |
| **Mean Reciprocal Rank (MRR)** | 0.327 | 0.005 |
| **Median Rank** | 6 / 200 | 100 / 200 |
| **Separation Gap** | 0.509 | 0.0 |
| **EEG‚ÜíText MRR** | 0.327 | 0.005 |
| **Text‚ÜíEEG MRR** | 0.287 | 0.005 |

### Training Set Performance (S1+S2, ~6,400 samples)

| Metric | Value |
|--------|-------|
| **Top-1 Accuracy** | 34.0% |
| **Top-5 Accuracy** | 74.0% |
| **MRR** | 0.508 |
| **Median Rank** | 3 / 200 |
| **Separation Gap** | 0.670 |

### Recall Performance (Test Set)

| Metric | Value | Interpretation |
|--------|-------|----------------|
| Recall@1 | 17.0% | Correct text is top prediction |
| Recall@5 | 47.5% | Correct text in top 5 |
| Recall@10 | 68.5% | Correct text in top 10 |
| Recall@20 | 85.5% | Correct text in top 20 |
| Recall@50 | 96.0% | Correct text in top 50 |

### Diagnostic Metrics

| Metric | Train | Test | Analysis |
|--------|-------|------|----------|
| **Mean Correct Similarity** | 0.673 | 0.510 | Strong positive signal retained |
| **Mean Incorrect Similarity** | 0.003 | 0.001 | Good negative separation |
| **Embedding Std** | 0.043 | 0.043 | Mode collapse detected (expected ~0.2) |
| **Permutation Baseline** | 0.002 | 0.005 | Model learned genuine patterns |

### Key Observations

‚úÖ **34x better than chance** on test set (0.17 vs 0.005)  
‚úÖ **Strong separation** between correct/incorrect pairs (gap = 0.51)  
‚úÖ **Meaningful learning** validated by permutation tests  
‚ö†Ô∏è **Significant overfitting** (34% ‚Üí 17%, likely due to only 2 subjects)  
‚ö†Ô∏è **Mode collapse** present (embedding std = 0.043 vs expected ~0.2) but model still discriminates  
‚ö†Ô∏è **Asymmetric retrieval** (EEG‚ÜíText better than Text‚ÜíEEG)  

**Interpretation:** Despite overfitting and mode collapse, the model successfully learned subject-invariant semantic features from cross-subject training, as evidenced by strong separation gaps and performance well above permutation baselines. The challenges indicate need for more subjects and architectural improvements (diversity regularization, attention pooling).

---

## üèóÔ∏è Architecture

### Dual-Encoder Contrastive Learning (CLIP-inspired)
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        EEG Branch                           ‚îÇ
‚îÇ  Raw EEG (125 channels, 1650 timesteps)                    ‚îÇ
‚îÇ       ‚Üì                                                      ‚îÇ
‚îÇ  CNN Feature Extractor                                      ‚îÇ
‚îÇ    Conv1D: 125‚Üí256 (kernel=10, stride=4)                   ‚îÇ
‚îÇ    Conv1D: 256‚Üí512 (kernel=3, stride=2)                    ‚îÇ
‚îÇ    Output: (512, ~206 timesteps)                           ‚îÇ
‚îÇ       ‚Üì                                                      ‚îÇ
‚îÇ  Linear Projection: 512 ‚Üí 512                              ‚îÇ
‚îÇ       ‚Üì                                                      ‚îÇ
‚îÇ  Positional Encoding (learned)                             ‚îÇ
‚îÇ       ‚Üì                                                      ‚îÇ
‚îÇ  Transformer Encoder (2 layers, 8 heads, key_dim=64)      ‚îÇ
‚îÇ       ‚Üì                                                      ‚îÇ
‚îÇ  Temporal Pooling (mean across time)                       ‚îÇ
‚îÇ       ‚Üì                                                      ‚îÇ
‚îÇ  Linear Projection + L2 Norm ‚Üí EEG Embedding (512-D)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Text Branch                          ‚îÇ
‚îÇ  Chinese Text (6-15 characters)                            ‚îÇ
‚îÇ       ‚Üì                                                      ‚îÇ
‚îÇ  BERT Tokenizer                                            ‚îÇ
‚îÇ       ‚Üì                                                      ‚îÇ
‚îÇ  BERT-base-chinese (110M parameters)                       ‚îÇ
‚îÇ       ‚Üì                                                      ‚îÇ
‚îÇ  [CLS] Token Extraction (768-D)                            ‚îÇ
‚îÇ       ‚Üì                                                      ‚îÇ
‚îÇ  Linear Projection: 768 ‚Üí 512                              ‚îÇ
‚îÇ       ‚Üì                                                      ‚îÇ
‚îÇ  L2 Normalization ‚Üí Text Embedding (512-D)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

              ‚Üì                    ‚Üì
         [Cosine Similarity Matrix (B √ó B)]
                    ‚Üì
    [Bidirectional Contrastive Loss (CLIP)]
    (EEG‚ÜíText + Text‚ÜíEEG) / 2
```

### Technical Contributions

**Novel for imagined speech decoding:**
1. **Cross-subject contrastive learning**: Training on mixed S1+S2 data without subject IDs, forcing model to learn subject-invariant features
2. **Sentence-level retrieval**: 200-way open-vocabulary matching vs traditional classification
3. **Contrastive EEG-text alignment**: Applying CLIP framework to neural-linguistic cross-modal learning

**Engineering optimizations:**
1. **Hierarchical CNN downsampling**: Reduces 1650 timesteps to ~206 features for efficient Transformer processing
2. **Memory-efficient pipeline**: Lazy loading + streaming normalization
3. **Dual-encoder architecture**: Enables fast retrieval via pre-computed text embeddings

---

## üß† Understanding Neural Embeddings & Contrastive Learning

### What Are Embeddings?

**Embeddings** are fixed-length vector representations that capture semantic meaning:
```python
# Text embedding (512 numbers)
"‰ªäÂ§©ÁöÑÊôöÈ§êÂæàÂ•ΩÂêÉ" ‚Üí [0.23, -0.45, 0.67, ..., 0.12]  # 512-D vector

# EEG embedding (512 numbers)  
EEG signal (125√ó1650 = 206,250 numbers) ‚Üí [0.21, -0.43, 0.65, ..., 0.10]  # 512-D
```

**Key insight:** If the text and EEG represent the **same semantic content**, their embeddings should be **close together** in 512-dimensional space (high cosine similarity).

---

### How Contrastive Learning Works (CLIP Loss)

Our model learns by comparing **pairs** of EEG and text embeddings in each batch:
```
Batch of 32 samples:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ EEG Signal  ‚îÇ Text         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ EEG_1       ‚îÇ "‰ªäÂ§©ÂæàÂ•Ω"    ‚îÇ ‚Üê Matched pair (should be similar)
‚îÇ EEG_2       ‚îÇ "ÊàëË¶ÅÂê¨Èü≥‰πê‰ºö"‚îÇ ‚Üê Matched pair
‚îÇ ...         ‚îÇ ...          ‚îÇ
‚îÇ EEG_32      ‚îÇ "ÊòéÂ§©Â§©Ê∞îÂ•Ω"   ‚îÇ ‚Üê Matched pair
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Create similarity matrix (32 √ó 32):
                Text_1  Text_2  ...  Text_32
EEG_1           0.89    0.12         0.05     ‚Üê HIGH for diagonal (correct pairs)
EEG_2           0.11    0.87         0.08     
...
EEG_32          0.06    0.09         0.91

Loss function:
- Maximize diagonal (correct EEG‚ÜîText pairs)
- Minimize off-diagonal (incorrect pairs)
- Bidirectional: Compute both EEG‚ÜíText and Text‚ÜíEEG
```

### The Loss Function
```python
# Compute similarity matrix
logits = temperature * (EEG_embeddings @ Text_embeddings.T)  # (B, B) matrix

# Labels: diagonal indices [0, 1, 2, ..., B-1]
labels = torch.arange(B)

# Bidirectional loss
loss_eeg_to_text = CrossEntropy(logits, labels)
loss_text_to_eeg = CrossEntropy(logits.T, labels)
total_loss = (loss_eeg_to_text + loss_text_to_eeg) / 2
```

**Why this works:**
- **Positive pairs** (EEG_i ‚Üî Text_i): Pulled together in embedding space
- **Negative pairs** (EEG_i ‚Üî Text_j, i‚â†j): Pushed apart
- **Subject-invariance emerges**: Model learns semantic meaning, ignoring subject-specific noise

---

### Why Embeddings Enable Cross-Subject Learning

**Traditional approach (subject-specific):**
```python
S1_EEG ‚Üí S1_specific_decoder ‚Üí Text  ‚úì (works only for S1)
S2_EEG ‚Üí S1_specific_decoder ‚Üí Text  ‚úó (fails - different patterns)
```

**Our approach (shared embedding space):**
```python
# Universal semantic space
S1_EEG ‚Üí Shared Encoder ‚Üí 512-D embedding ‚Üí Matches text embedding
S2_EEG ‚Üí Shared Encoder ‚Üí 512-D embedding ‚Üí Matches text embedding

# The encoder learns: "extract meaning, ignore subject-specific variation"
```

**The mechanism:** By forcing S1 and S2's EEG signals for the **same sentence** to have similar embeddings as the text, the model learns subject-invariant features. Despite r=0.13 correlation between S1 and S2's raw signals, the learned embeddings capture the shared 13% semantic structure.

---

### Embedding Quality Analysis

From our results:

| Metric | Train | Test | Interpretation |
|--------|-------|------|----------------|
| **Correct Similarity** | 0.673 | 0.510 | Matched pairs are well-aligned |
| **Incorrect Similarity** | 0.003 | 0.001 | Mismatched pairs nearly orthogonal |
| **Separation Gap** | 0.670 | 0.509 | Strong discrimination maintained |
| **Embedding Std** | 0.043 | 0.043 | Mode collapse detected (expected ~0.2) |

**Key findings:**
‚úÖ Strong separation between correct/incorrect pairs (gap > 0.5)  
‚úÖ Model learned discriminative features despite mode collapse  
‚ö†Ô∏è Low embedding diversity suggests over-compression in 512-D space  
‚ö†Ô∏è Future work: diversity regularization to increase embedding spread  

---

### Practical Implications for BCIs

**1. Thought-to-Speech Systems**
```python
# Real-time inference
eeg_embedding = model.encode_eeg(live_eeg_signal)  # 512-D
similarities = eeg_embedding @ text_corpus_embeddings.T
predicted_sentence = text_corpus[similarities.argmax()]
# ‚Üí Synthesize speech from predicted sentence
```

**2. Agent Communication**
```python
# Instead of text retrieval, use embeddings for:
thought_embedding = model.encode_eeg(eeg_signal)  # 512-D

# Option A: Retrieve pre-defined commands
command = find_nearest_command(thought_embedding)

# Option B: Generate text via decoder (like LLMs use word embeddings)
# thought_embedding ‚Üí Decoder ‚Üí Generated text sequence
# Future work: Train decoder to generate from neural embeddings
```

**3. Brain-to-Brain Communication**
```python
# Person A's thought
embedding_A = encode_eeg(person_A_signal)  # 512-D

# Transmit embedding (512 numbers, highly compressed)
# Person B receives embedding

# Decode to Person B's understanding
text_B = retrieve_text(embedding_A, person_B_vocabulary)
# Or stimulate Person B's brain to recreate similar embedding
```

**Key advantage:** Neural embeddings capture **semantic meaning** independent of:
- Surface form (exact words)
- Individual brain patterns (subject-invariant)

This enables true "thought-level" communication beyond traditional text/speech.

---

## üî¨ The Cross-Subject Challenge

### Why is r=0.13 (S1-S2) correlation so hard?

EEG signals are **~87% subject-specific**. The same sentence produces vastly different neural patterns across individuals:
```python
Both subjects imagining: "‰ªäÂ§©ÁöÑÊôöÈ§êÂæàÂ•ΩÂêÉ"

Subject S1's EEG: [0.23, -0.45, 0.67, -0.12, 0.89, ...] (125 channels)
Subject S2's EEG: [0.15, -0.33, 0.51, -0.08, 0.72, ...]

Pearson correlation: r = 0.13 (only 13% shared variance!)
```

**What this means:**
- **87% of the signal** is subject-specific noise (skull thickness, electrode placement, neural anatomy)
- **13% of the signal** carries shared semantic information
- Traditional ML would overfit to subject-specific patterns and fail on new subjects

**Our solution:** Contrastive learning forces the model to:
1. Extract the 13% semantic signal
2. Ignore the 87% subject-specific noise
3. Align both subjects' embeddings with the same text embedding

**Proof it worked:**
- Permutation test: 0.17 (real) vs 0.005 (shuffled) ‚Üí Learning genuine EEG-text associations
- Separation gap: 0.51 ‚Üí Model discriminates semantic categories
- Cross-subject test accuracy 34x better than chance ‚Üí Subject-invariant features learned

---

## üöÄ Quick Start

### Installation
```bash
git clone https://github.com/HuzyafaK/ChiSCO-CrossSubject-Decoder.git
cd ChiSCO-CrossSubject-Decoder
pip install -r requirements.txt
```

### Download ChiSCO Dataset
```bash
# Visit: https://openneuro.org/datasets/ds005170
# Download preprocessed .pkl files for subjects 1-3
# Place in data/ directory
```

### Training
```bash
python src/train.py \
    --data_glob "data/**/*.pkl" \
    --subjects S1 S2 \
    --batch_size 32 \
    --epochs 30 \
    --lr_eeg 1e-4 \
    --lr_bert 2e-5 \
    --latent_dim 512 \
    --checkpoint_path "chisco_clip_best_model.pt"
```

### Evaluation
```bash
python src/evaluate.py \
    --checkpoint_path "chisco_clip_best_model.pt" \
    --test_data "data/"
```

---

## üîß Memory-Efficient Design

Key optimizations for working with large EEG datasets on limited hardware:

### 1. Lazy Loading
```python
# Instead of loading all tensors into memory:
all_eeg = []  # Would need 28GB RAM
for trial in dataset:
    all_eeg.append(load_eeg(trial))  # OOM!

# We store only file paths:
indexes = [{'path': 'data/s1.pkl', 'trial_idx': 0}, ...]  # ~1MB
# Load on-demand during training
```

### 2. Streaming Normalization
```python
# Compute mean/std without loading full dataset:
sum_channels = torch.zeros(125)
count = 0
for entry in indexes:
    segment = load_segment(entry)  # Load one at a time
    sum_channels += segment.mean(dim=1)
    count += 1
mean = sum_channels / count  # Never held full data in memory
```

### 3. Chunked Gallery Encoding
```python
# During evaluation, encode gallery via DataLoader:
gallery_embeddings = []
for batch in DataLoader(gallery, batch_size=32):
    embeddings = model.encode(batch)
    gallery_embeddings.append(embeddings.cpu())
# Avoid creating N√óN similarity matrix (would be 200√ó200√ó4 bytes = 160KB, but for full dataset: GB scale)
```

**Impact:** Reduced peak memory, enabling training on single GPU.

---

## üß™ Diagnostic Analysis

### Permutation Test
- **Model performance:** 17% (test), 34% (train)
- **Shuffled baseline:** 0.5% (test), 0.2% (train)
- **Conclusion:** Model learned genuine EEG-text associations, not spurious correlations ‚úÖ

### Overfitting Analysis
- **Train:** 34% top-1, 74% top-5, 0.67 separation gap
- **Test:** 17% top-1, 47.5% top-5, 0.51 separation gap
- **Drop:** 50% in top-1, but separation gap only dropped 24%
- **Interpretation:** Significant overfitting due to limited subjects (only S1+S2), but model still generalizes meaningfully as shown by maintained separation and far-above-chance performance

### Mode Collapse Detection
- **Observed embedding std:** 0.043 (both train and test)
- **Expected for 512-D:** ~0.2
- **Diagnosis:** Representations compressed into narrow subspace
- **Impact:** Despite collapse, model still achieves 0.51 separation gap ‚Üí learned features are discriminative, just not diverse

**Key insight:** Overfitting and mode collapse coexist, but model learned useful subject-invariant features evidenced by:
- Strong separation gaps (0.51-0.67)
- Recall@50: 96% (test) - model retrieves correct answer in top 25%
- Permutation tests: 34x better than shuffled baseline

---

## ‚ö†Ô∏è Important Notes

### Research Context

This is an **independent research project** for my startup Excelleve. It uses the publicly available ChiSCO dataset but is:

- **Not affiliated** with original ChiSCO authors or Harbin Institute of Technology
- **Not endorsed** by the dataset creators
- An **internal MVP** validated against existing literature
- **Properly attributed** to Zhang et al. (2024)

### Novel Contributions

**To the best of our knowledge**, no prior work demonstrates cross-subject sentence-level imagined speech decoding using contrastive retrieval. What's new:

1. **Cross-subject approach**: Mixed S1+S2 training without subject IDs
2. **Contrastive EEG-text alignment**: CLIP-inspired dual-encoder for neural-linguistic learning
3. **200-way sentence retrieval**: Open-vocabulary matching vs classification
4. **Subject-invariant learning**: Achieving 34x better than chance despite r=0.13 correlation

The ChiSCO paper focused on subject-specific semantics classification (25-29% on 39-way task). This work tackles cross-subject generalization‚Äîcritical for practical BCI deployment.

---

## üìö Dataset

**ChiSCO (Chinese Imagined Speech Corpus)**
- **Paper:** [Zhang et al., Nature Scientific Data, 2024](https://www.nature.com/articles/s41597-024-04114-1)
- **Repository:** [OpenNeuro ds005170](https://openneuro.org/datasets/ds005170)
- **Subjects:** 3 (S1: F 26y, S2: M 30y, S3: M 22y)
- **Trials:** 6,681 per subject (900+ min recording time)
- **Task:** Read sentence (5s) ‚Üí Imagine speaking (3.3s)
- **EEG:** 125 channels, 500Hz sampling rate
- **Language:** Chinese (6-15 characters, 39 semantic categories)

**Inter-subject correlations** (from ChiSCO paper):
- r(S1, S2) = 0.126
- r(S2, S3) = 0.147
- r(S1, S3) = 0.168

**Our setup:** Trained on S1+S2 (r=0.13 correlation between these two subjects)

---

## üõ†Ô∏è Technical Stack

- **Deep Learning:** PyTorch 2.0+
- **NLP:** Hugging Face Transformers (BERT-base-chinese, 110M params)
- **EEG Processing:** NumPy, pickle
- **Metrics:** NLTK (BLEU), Rouge, scikit-learn
- **Utilities:** tqdm, argparse

---

## üéØ Future Work

### Immediate Improvements (Expected +8-12% accuracy)
1. **Diversity regularization**: Add `-std(embeddings)` penalty to prevent mode collapse
2. **Attention-based pooling**: Replace mean pooling with learned attention weights to capture discriminative temporal features
3. **Temperature scheduling**: Anneal from 0.07 ‚Üí 0.03 during training for better convergence
4. **More subjects**: Extend to S3 and additional subjects for robust cross-subject learning

### Medium-term Directions
1. **Subject adaptation**: Fine-tune on 5-10 minutes of new user data (vs 900+ min from scratch)
2. **Asymmetric loss weighting**: Address Text‚ÜíEEG being weaker than EEG‚ÜíText
3. **Hard negative mining**: Focus training on difficult examples
4. **Data augmentation**: Temporal masking, channel dropout for EEG; back-translation for text

### Long-term Vision
1. **Self-supervised pretraining**: Leverage unlabeled EEG data
2. **Real-time deployment**: Optimize for low-latency inference (<50ms)
3. **Generative decoding**: Train transformer decoder to generate text from neural embeddings (enabling open-ended thought-to-speech)
4. **Agent communication**: Use embeddings as interface between thought and AI systems

**Target performance:** 30-35% top-1 accuracy with above improvements on expanded subject pool

---

## üìñ Citation
```bibtex
@software{khokhar2026crosssubject,
  author = {Khokhar, Muhammad Huzyafa},
  title = {Cross-Subject Imagined Speech EEG-to-Text Decoder},
  year = {2026},
  publisher = {GitHub},
  url = {https://github.com/HuzyafaK/ChiSCO-CrossSubject-Decoder},
  note = {Internal research MVP for Excelleve}
}
```

### ChiSCO Dataset Citation

**This work builds upon the ChiSCO dataset:**
```bibtex
@article{zhang2024chisco,
  title={Chisco: An EEG-based BCI dataset for decoding of imagined speech},
  author={Zhang, Zihan and Ding, Xiao and Bao, Yu and Zhao, Yi and Liang, Xia and Qin, Bing and Liu, Ting},
  journal={Scientific Data},
  volume={11},
  number={1},
  pages={1265},
  year={2024},
  publisher={Nature Publishing Group UK London},
  doi={10.1038/s41597-024-04114-1}
}
```

---

## üôè Acknowledgments

**ChiSCO Dataset Creators:**
- Zihan Zhang, Xiao Ding, Yu Bao, Yi Zhao, Bing Qin, Ting Liu (Harbin Institute of Technology)
- Xia Liang (Northeast Forestry University)

Their pioneering work on imagined speech paradigms and rigorous data collection (900+ minutes per subject, attention checks, quality controls) made this cross-subject research possible.

**Additional:**
- East China Normal University Xing Tian EEG Lab for equipment access and demo support
- OpenAI CLIP paper (Radford et al., 2021) for contrastive learning framework inspiration
- Hugging Face for Transformers library

---

## üìÑ License

MIT License - See [LICENSE](LICENSE) for details

---

**Built with üß† by [Muhammad Huzyafa Khokhar](https://mhkhokhar.com) | [Excelleve](https://excelleve.com)**

*Pioneering non-invasive thought-to-speech systems*
