# Cross-Subject Imagined Speech EEG-to-Text Decoder

**First cross-subject contrastive learning system for sentence-level imagined speech decoding**

---

## ğŸ¯ Key Achievement

Achieved **17% top-1 accuracy** on 200-way EEG-to-text retrieval (34x better than chance) despite extremely low inter-subject correlation (r=0.13-0.17) on the ChiSCO imagined speech dataset.

**Why this matters:** Most EEG decoders require 900+ minutes of per-user calibration. This cross-subject approach demonstrates that subject-invariant features can be learned, potentially reducing calibration requirements to minutes rather than hoursâ€”making BCIs more practical for real-world deployment.

## Built on the ChiSCO Dataset

This work builds upon the **ChiSCO (Chinese Imagined Speech Corpus)** dataset created by **Zhang et al. (2024)** at **Harbin Institute of Technology**â€”the first dataset for **sentence-level imagined speech** with **6,681 trials per subject**.Â The ChiSCO teamâ€™s rigorous data collection (**900+ minutes per subject, attention checks, quality controls**) and their pioneering work on imagined speech paradigms made this cross-subject research possible.

In this project, we specifically used data from subjects **S1 and S2**.

**Key insight from the ChiSCO paper:** Inter-subject correlations of **0.13â€“0.17** indicate that EEG signals are ~**87% subject-specific**â€”making cross-subject learning the critical challenge for practical BCIs.

---

## ğŸ“Š Performance Metrics

| Metric                     | Value         | Baseline      |
| -------------------------- | ------------- | ------------- |
| Top-1 Accuracy             | **17.0%**     | 0.5% (chance) |
| Top-5 Accuracy             | **47.5%**     | 2.5% (chance) |
| Mean Reciprocal Rank (MRR) | **0.327**     | 0.005         |
| Median Rank                | **6 / 200**   | 100 / 200     |
| Separation Gap             | **0.509**     | 0.0           |
| Inter-subject Correlation  | **0.13â€“0.17** | N/A           |

* **Training set:** 34% top-1 (proof of learning)
* **Test set:** 17% top-1 (generalization with overfitting due to only 2 subjects)

### Recall Performance

| Metric    | Value |
| --------- | ----- |
| Recall@1  | 17.0% |
| Recall@5  | 47.5% |
| Recall@10 | 68.5% |
| Recall@20 | 85.5% |
| Recall@50 | 96.0% |

---

## ğŸ—ï¸ Architecture

### Dual-Encoder Contrastive Learning (CLIP-inspired)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        EEG Branch                           â”‚
â”‚  Raw EEG (125, 1650)                                        â”‚
â”‚       â†“                                                      â”‚
â”‚  CNN Feature Extractor (125â†’256â†’512 channels)               â”‚
â”‚       â†“                                                      â”‚
â”‚  Time Projection (512 â†’ 512)                                â”‚
â”‚       â†“                                                      â”‚
â”‚  Positional Encoding                                        â”‚
â”‚       â†“                                                      â”‚
â”‚  Transformer Encoder (2 layers, 8 heads)                    â”‚
â”‚       â†“                                                      â”‚
â”‚  Temporal Pooling (mean)                                    â”‚
â”‚       â†“                                                      â”‚
â”‚  L2 Normalization â†’ EEG Embedding (512-D)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Text Branch                          â”‚
â”‚  Chinese Text                                               â”‚
â”‚       â†“                                                      â”‚
â”‚  BERT-base-chinese (110M params)                            â”‚
â”‚       â†“                                                      â”‚
â”‚  [CLS] Token + Projection (768 â†’ 512)                       â”‚
â”‚       â†“                                                      â”‚
â”‚  L2 Normalization â†’ Text Embedding (512-D)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

              â†“                    â†“
         [Cosine Similarity (B Ã— B)]
                    â†“
           [Contrastive CLIP Loss]
```

### Key Innovations

* **Hierarchical CNN:** Conv1D stride 4â†’2 downsamples 1650 timesteps to ~206 features
* **Cross-subject learning:** Trained on mixed subjects without subject IDs
* **Memory efficiency:** Lazy loading + streaming normalization (**87% memory reduction**)
* **Contrastive alignment:** EEG and text embeddings in shared **512-D** space

---

## ğŸ§  Understanding Neural Embeddings & Contrastive Learning

### What Are Embeddings?

**Embeddings** are fixed-length vector representations that capture semantic meaning:

```python
# Text embedding (512 numbers)
"ä»Šå¤©çš„æ™šé¤å¾ˆå¥½åƒ" â†’ [0.23, -0.45, 0.67, ..., 0.12]

# EEG embedding (512 numbers)
EEG signal (125 channels Ã— 1650 timesteps) â†’ [0.21, -0.43, 0.65, ..., 0.10]
```

**Key insight:** If the text and EEG represent the **same semantic content**, their embeddings should be **close together** in the 512-dimensional space.

---

### How Contrastive Learning Works (CLIP Loss)

Our model learns by comparing **pairs** of EEG and text embeddings:

```
Batch of 32 samples:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EEG Signal  â”‚ Text         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ EEG_1       â”‚ "ä»Šå¤©å¾ˆå¥½"    â”‚
â”‚ EEG_2       â”‚ "æˆ‘è¦å»å¬éŸ³ä¹ä¼š"â”‚
â”‚ ...         â”‚ ...          â”‚
â”‚ EEG_32      â”‚ "æ˜å¤©å¤©æ°”å¥½"   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Similarity matrix (32 Ã— 32):
                Text_1  Text_2  ...  Text_32
EEG_1           0.89    0.12         0.05
EEG_2           0.11    0.87         0.08
...
EEG_32          0.06    0.09         0.91
```

Goal: **maximize diagonal (correct pairs)**, **minimize off-diagonal (wrong pairs)**.

---

### The Loss Function

```python
similarity_scores = EEG_embedding @ Text_embeddings.T
scaled_logits = temperature * similarity_scores
loss = CrossEntropy(scaled_logits, diagonal_labels)

total_loss = (EEG_to_Text_loss + Text_to_EEG_loss) / 2
```

**Why this works:**

* Positive pairs are pulled together
* Negative pairs are pushed apart
* **Subject-invariance emerges**

---

### Visualization of Learned Embeddings

```
512-D Embedding Space (simplified to 2D)

   Food & Dining
     EEGâ‚ â—â”€â”€â— Textâ‚

   Music & Arts
     EEGâ‚‚ â—â”€â”€â— Textâ‚‚

   Weather
     EEGâ‚ƒâ‚‚ â—â”€â”€â— Textâ‚ƒâ‚‚
```

---

### Why Embeddings Enable Cross-Subject Learning

**Traditional (subject-specific):**

```python
S1_EEG â†’ S1_decoder â†’ Text
S2_EEG â†’ S1_decoder â†’ âœ—
```

**Our approach (shared embedding space):**

```python
S1_EEG â†’ Shared Encoder â†’ 512-D â†’ Text
S2_EEG â†’ Shared Encoder â†’ 512-D â†’ Text
```

---

### Embedding Quality Metrics

| Metric                    | Value | Interpretation         |
| ------------------------- | ----- | ---------------------- |
| Mean Correct Similarity   | 0.51  | Strong alignment       |
| Mean Incorrect Similarity | 0.001 | Near-orthogonal        |
| Separation Gap            | 0.509 | Clear discrimination   |
| Embedding Std             | 0.042 | Mode collapse detected |

---

### Mathematical Formulation

**Cosine Similarity:**

```
similarity = (EEG Â· Text) / (||EEG|| Ã— ||Text||)
```

**Contrastive Loss (InfoNCE):**

```
exp(sim(EEG_i, Text_i) / Ï„)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Î£â±¼ exp(sim(EEG_i, Text_j) / Ï„)
```

---

### Why 512 Dimensions?

* Compression of 206k EEG values
* Efficient cosine similarity
* Proven by CLIP
* Balanced capacity vs overfitting

---

## ğŸ”¬ The Cross-Subject Challenge

EEG signals are ~**87% subject-specific**. Same sentence, different neural patterns:

```python
Subject 1: [0.23, -0.45, 0.67, ...]
Subject 2: [0.15, -0.33, 0.51, ...]
Correlation: 0.13
```

**Solution:** Contrastive learning extracts subject-invariant semantic meaning.

---

## ğŸš€ Quick Start

### Installation

```bash
git clone https://github.com/HuzyafaK/ChiSCO-CrossSubject-Decoder.git
cd ChiSCO-CrossSubject-Decoder
pip install -r requirements.txt
```

### Download ChiSCO Dataset

```bash
# Visit: https://openneuro.org/datasets/ds005170
# Place files in data/ directory
```

### Training

```bash
python src/train.py \
    --data_glob "data/**/*.pkl" \
    --subjects S1 S2 \
    --batch_size 32 \
    --epochs 30 \
    --lr_eeg 1e-4 \
    --lr_bert 2e-5
```

### Evaluation

```bash
python src/evaluate.py \
    --checkpoint_path chisco_clip_best_model.pt
```

---

## ğŸ”§ Memory-Efficient Design

* Lazy loading (paths instead of tensors)
* Streaming normalization
* Chunked encoding via DataLoader

**Impact:** 28GB â†’ **3.6GB peak memory** (**87% reduction**)

---

## ğŸ§ª Diagnostic Analysis

**Permutation Test:**

* Model: 17%
* Shuffled: 0.5%

**Mode Collapse Detection:**

* Embedding std: 0.042 (expected ~0.2)

---

## âš ï¸ Important Notes

### Research Context

This is an independent research project for my startup **Excelleve**. It uses the publicly available **ChiSCO dataset** but is:

* Not affiliated with original ChiSCO authors or Harbin Institute of Technology
* Not endorsed by the dataset creators
* An independent implementation of cross-subject learning
* Properly attributed to **Zhang et al. (2024)**

### Novel Contributions

What's new in this work (not in original ChiSCO paper):

* ****Cross-subject learning:**** Training on mixed subjects (S1+S2) without subject IDs, learning subject-invariant features despite r=0.13 correlation
* ****Contrastive learning:**** CLIP-inspired dual-encoder for EEG-text alignment in shared embedding space
* ****200-way retrieval:**** Open-vocabulary sentence matching (vs 39-way classification)
* ****Memory efficiency:**** Lazy loading for resource-constrained training

**Impact:** Demonstrates feasibility of reduced-calibration BCIsâ€”future work includes subject adaptation where new users would need only 5-10 minutes of data (vs 900+ minutes for subject-specific models).

## ğŸ“š Dataset

**ChiSCO (Chinese Imagined Speech Corpus)**

* Paper: *Nature Scientific Data*, 2024
* Repository: OpenNeuro ds005170
* Size: 6,681 trials Ã— 3 subjects
* EEG: 125 channels, 500 Hz
* Language: Chinese (6â€“15 characters)

---

## ğŸ› ï¸ Technical Stack

* PyTorch 2.0+
* Hugging Face Transformers (BERT-base-chinese)
* NumPy, tqdm, pickle
* NLTK (BLEU), Rouge

---

## ğŸ¯ Future Work

### Immediate

* Diversity loss to prevent mode collapse
* Attention-based pooling
* Temperature scheduling

### Long-term

* Subject adaptation (5-min fine-tuning)
* Self-supervised pretraining
* Multi-modal fusion

**Expected:** 25â€“30% top-1 accuracy

---

## ğŸ“– Citation

```bibtex
@software{khokhar2025crosssubject,
  author = {Khokhar, Muhammad Huzyafa},
  title = {Cross-Subject Imagined Speech EEG-to-Text Decoder},
  year = {2025},
  publisher = {GitHub},
  url = {https://github.com/HuzyafaK/ChiSCO-CrossSubject-Decoder}
}
```

### ChiSCO Dataset Citation

```bibtex
@article{zhang2024chisco,
  title={Chisco: An EEG-based BCI dataset for decoding of imagined speech},
  author={Zhang, Zihan and Ding, Xiao and Bao, Yu and Zhao, Yi and Liang, Xia and Qin, Bing and Liu, Ting},
  journal={Scientific Data},
  volume={11},
  number={1},
  pages={1265},
  year={2024},
  publisher={Nature Publishing Group UK London},
  doi={10.1038/s41597-024-04114-1}
}
```

---

## ğŸ™ Acknowledgments

**ChiSCO Dataset Creators:**

* Zihan Zhang, Xiao Ding, Yu Bao, Yi Zhao, Bing Qin, Ting Liu (Harbin Institute of Technology)
* Xia Liang (Northeast Forestry University)

**Additional:**

* East China Normal University Xing Tian EEG Lab
* OpenAI CLIP (Radford et al., 2021)
* Hugging Face Transformers

---

## ğŸ“„ License

MIT License â€” see `LICENSE`.

---

**Built with ğŸ§  by Muhammad Huzyafa Khokhar | Excelleve**
*Pioneering non-invasive thought-to-speech systems*
